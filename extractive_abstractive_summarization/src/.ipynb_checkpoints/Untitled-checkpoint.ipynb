{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "trainData=np.load('./train.pkl',allow_pickle=True)\n",
    "valData=np.load('./valid.pkl',allow_pickle=True)\n",
    "testData=np.load('./test.pkl',allow_pickle=True)\n",
    "embedding=np.load('./embedding.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=embedding.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self,trainData,max_len=300):\n",
    "        trainX=[]\n",
    "        trainY=[]\n",
    "        bound=[]\n",
    "        padding_idx=trainData.padding\n",
    "        for i in trainData:\n",
    "            text=i['text']\n",
    "            label=i['label']\n",
    "            bod=i['sent_range']\n",
    "            if(len(text)<max_len):\n",
    "                text=text+[padding_idx]*(max_len-len(text))\n",
    "                label=label+[padding_idx]*(max_len-len(label))\n",
    "            trainX.append(text)\n",
    "            label=to_categorical(label)\n",
    "            trainY.append(label)\n",
    "        self.trainX=trainX\n",
    "        self.trainY=trainY\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trainX)\n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx=idx.tolist()\n",
    "        \n",
    "        return np.array(self.trainX[idx],dtype=np.long),np.array(self.trainY[idx],dtype=np.float)\n",
    "    \n",
    "    def get_bound(self,idx,batch_size):\n",
    "        return self.bound[idx*batch_size:(idx+1)*batch_size],self.extract[idx*batch_size:(idx+1)*batch_size]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ExtractiveSeqTagging(nn.Module):\n",
    "    def __init__(self,hidden_size,pretrain_embedding,bidirectional=True):\n",
    "        super(ExtractiveSeqTagging,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.bidirectional=bidirectional\n",
    "        self.embedding=nn.Embedding.from_pretrained(pretrain_embedding)\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size,bidirectional=bidirectional)\n",
    "        if bidirectional==True:\n",
    "            self.out=nn.Linear(hidden_size*2,2)\n",
    "    \n",
    "    def forward(self,x,hidden):\n",
    "        embedded=self.embedding(x).transpose(0,1)\n",
    "        out,_=self.gru(embedded,hidden)\n",
    "        out=self.out(out)\n",
    "        return out\n",
    "#     h_0 of shape (num_layers * num_directions, batch, hidden_size): \n",
    "    def initHidden(self,batch):\n",
    "        shape=(1,batch,self.hidden_size)\n",
    "        if self.bidirectional==True:\n",
    "            shape=(2,batch,self.hidden_size)\n",
    "            \n",
    "        return torch.zeros(shape,device='cuda')\n",
    "    def predict(self,x,hidden):\n",
    "        embedded=self.embedding(x).transpose(0,1)\n",
    "        \n",
    "        \n",
    "        out=self.gru(embedded,hidden)\n",
    "        out=self.out(out)\n",
    "        out=nn.functional.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score.rouge_scorer import RougeScorer\n",
    "from multiprocessing import Pool,cpu_count\n",
    "\n",
    "def calculate_rouge_score(prediction,target):\n",
    "    ROUGE_TYPES = ['rouge1', 'rouge2', 'rougeL']\n",
    "    USE_STEMMER = False\n",
    "\n",
    "\n",
    "    rouge_scorer = RougeScorer(ROUGE_TYPES, use_stemmer=USE_STEMMER)\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        scores = pool.starmap(rouge_scorer.score,\n",
    "                            [(t, p) for t, p in zip(target, prediction)])\n",
    "\n",
    "    r1s = np.array([s['rouge1'].fmeasure for s in scores])\n",
    "    r2s = np.array([s['rouge2'].fmeasure for s in scores])\n",
    "    rls = np.array([s['rougeL'].fmeasure for s in scores])\n",
    "    scores = {\n",
    "        'mean': {\n",
    "            'rouge-1': r1s.mean(),\n",
    "            'rouge-2': r2s.mean(),\n",
    "            'rouge-l': rls.mean()\n",
    "        },\n",
    "        'std': {\n",
    "            'rouge-1': r1s.std(),\n",
    "            'rouge-2': r2s.std(),\n",
    "            'rouge-l': rls.std()\n",
    "        },\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "def predict2sentence(data,pred_arr):\n",
    "    target=[]\n",
    "    prediction=[]\n",
    "    assert len(pred_arr)==len(data)\n",
    "    for p,j in zip(pred_arr,data):\n",
    "        target.append(j['summary'])\n",
    "        sent_bounds = {i: bound for i, bound in enumerate(j['sent_bounds'])}\n",
    "        predict_sent=''\n",
    "        for sent_idx in p['predict_sentence_index']:\n",
    "            start, end = sent_bounds.get(sent_idx, (0, 0))\n",
    "            predict_sent += j['text'][start:end]\n",
    "        prediction.append(predict_sent)\n",
    "    return target,prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(trainData[12]['label'],num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1000012',\n",
       " 'label': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'sent_range': [(0, 14),\n",
       "  (14, 52),\n",
       "  (52, 69),\n",
       "  (69, 85),\n",
       "  (85, 112),\n",
       "  (112, 146),\n",
       "  (146, 179),\n",
       "  (179, 213),\n",
       "  (213, 238),\n",
       "  (238, 262),\n",
       "  (262, 286),\n",
       "  (286, 303),\n",
       "  (303, 325),\n",
       "  (325, 347)],\n",
       " 'text': [8470,\n",
       "  14513,\n",
       "  23,\n",
       "  178,\n",
       "  2930,\n",
       "  32,\n",
       "  349,\n",
       "  8,\n",
       "  33,\n",
       "  83,\n",
       "  20,\n",
       "  17059,\n",
       "  5,\n",
       "  3,\n",
       "  24,\n",
       "  47,\n",
       "  83,\n",
       "  1197,\n",
       "  55,\n",
       "  116,\n",
       "  671,\n",
       "  8,\n",
       "  62,\n",
       "  15435,\n",
       "  11,\n",
       "  5817,\n",
       "  4,\n",
       "  6,\n",
       "  3348,\n",
       "  17801,\n",
       "  23,\n",
       "  5421,\n",
       "  7,\n",
       "  32085,\n",
       "  4,\n",
       "  68,\n",
       "  70,\n",
       "  799,\n",
       "  150,\n",
       "  8,\n",
       "  1407,\n",
       "  58,\n",
       "  52,\n",
       "  5530,\n",
       "  15671,\n",
       "  11,\n",
       "  285,\n",
       "  9,\n",
       "  6,\n",
       "  5005,\n",
       "  5,\n",
       "  3,\n",
       "  6,\n",
       "  17061,\n",
       "  47,\n",
       "  103,\n",
       "  6,\n",
       "  2093,\n",
       "  47,\n",
       "  83,\n",
       "  10,\n",
       "  12,\n",
       "  462,\n",
       "  219,\n",
       "  12,\n",
       "  15,\n",
       "  150,\n",
       "  5,\n",
       "  3,\n",
       "  62,\n",
       "  803,\n",
       "  2930,\n",
       "  980,\n",
       "  8,\n",
       "  41,\n",
       "  250,\n",
       "  10,\n",
       "  245,\n",
       "  19,\n",
       "  143,\n",
       "  1997,\n",
       "  20,\n",
       "  74705,\n",
       "  5,\n",
       "  3,\n",
       "  11,\n",
       "  10,\n",
       "  2042,\n",
       "  27,\n",
       "  74705,\n",
       "  249,\n",
       "  11,\n",
       "  13420,\n",
       "  4,\n",
       "  6,\n",
       "  17061,\n",
       "  9568,\n",
       "  6,\n",
       "  2093,\n",
       "  23,\n",
       "  95,\n",
       "  15,\n",
       "  166,\n",
       "  61,\n",
       "  7180,\n",
       "  8,\n",
       "  150,\n",
       "  7,\n",
       "  62,\n",
       "  877,\n",
       "  5,\n",
       "  3,\n",
       "  12,\n",
       "  104,\n",
       "  108,\n",
       "  18,\n",
       "  104,\n",
       "  1296,\n",
       "  15,\n",
       "  31723,\n",
       "  81,\n",
       "  104,\n",
       "  197,\n",
       "  18,\n",
       "  104,\n",
       "  33,\n",
       "  217,\n",
       "  11,\n",
       "  46,\n",
       "  186,\n",
       "  481,\n",
       "  2156,\n",
       "  30,\n",
       "  518,\n",
       "  7,\n",
       "  30,\n",
       "  7180,\n",
       "  30,\n",
       "  74705,\n",
       "  4,\n",
       "  12,\n",
       "  103,\n",
       "  6,\n",
       "  17061,\n",
       "  5,\n",
       "  3,\n",
       "  12,\n",
       "  104,\n",
       "  108,\n",
       "  18,\n",
       "  204,\n",
       "  9,\n",
       "  133,\n",
       "  40,\n",
       "  1382,\n",
       "  24,\n",
       "  8129,\n",
       "  81,\n",
       "  46,\n",
       "  385,\n",
       "  7,\n",
       "  4075,\n",
       "  1300,\n",
       "  9,\n",
       "  2576,\n",
       "  440,\n",
       "  8,\n",
       "  41,\n",
       "  250,\n",
       "  233,\n",
       "  503,\n",
       "  7,\n",
       "  50,\n",
       "  33,\n",
       "  8,\n",
       "  587,\n",
       "  3650,\n",
       "  5,\n",
       "  3,\n",
       "  12,\n",
       "  31,\n",
       "  6,\n",
       "  1157,\n",
       "  9,\n",
       "  46,\n",
       "  574,\n",
       "  4,\n",
       "  1149,\n",
       "  21,\n",
       "  15,\n",
       "  314,\n",
       "  46,\n",
       "  877,\n",
       "  7,\n",
       "  74,\n",
       "  61,\n",
       "  1409,\n",
       "  81,\n",
       "  50,\n",
       "  2080,\n",
       "  125,\n",
       "  4,\n",
       "  30,\n",
       "  21,\n",
       "  49,\n",
       "  1481,\n",
       "  9,\n",
       "  1722,\n",
       "  195,\n",
       "  143,\n",
       "  5,\n",
       "  12,\n",
       "  3,\n",
       "  55,\n",
       "  103,\n",
       "  6,\n",
       "  2093,\n",
       "  80,\n",
       "  83,\n",
       "  60,\n",
       "  102,\n",
       "  175,\n",
       "  387,\n",
       "  7,\n",
       "  70,\n",
       "  230,\n",
       "  28,\n",
       "  41,\n",
       "  12,\n",
       "  11576,\n",
       "  462,\n",
       "  219,\n",
       "  15,\n",
       "  133,\n",
       "  204,\n",
       "  12,\n",
       "  5,\n",
       "  3,\n",
       "  12,\n",
       "  31723,\n",
       "  7,\n",
       "  104,\n",
       "  196,\n",
       "  757,\n",
       "  8,\n",
       "  3491,\n",
       "  242,\n",
       "  7,\n",
       "  242,\n",
       "  105,\n",
       "  6,\n",
       "  540,\n",
       "  134,\n",
       "  22,\n",
       "  71,\n",
       "  251,\n",
       "  4,\n",
       "  12,\n",
       "  55,\n",
       "  469,\n",
       "  5,\n",
       "  3,\n",
       "  6,\n",
       "  17061,\n",
       "  4921,\n",
       "  30,\n",
       "  10,\n",
       "  385,\n",
       "  7,\n",
       "  4075,\n",
       "  3855,\n",
       "  11,\n",
       "  13450,\n",
       "  170,\n",
       "  4,\n",
       "  300,\n",
       "  430,\n",
       "  62,\n",
       "  589,\n",
       "  27,\n",
       "  4703,\n",
       "  6,\n",
       "  1033,\n",
       "  16186,\n",
       "  5,\n",
       "  3,\n",
       "  55,\n",
       "  2238,\n",
       "  58,\n",
       "  62,\n",
       "  102,\n",
       "  4075,\n",
       "  11,\n",
       "  15693,\n",
       "  170,\n",
       "  248,\n",
       "  62,\n",
       "  102,\n",
       "  276,\n",
       "  4925]}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 8470, 14513,    23,   178,  2930,    32,   349,     8,    33,\n",
       "           83,    20, 17059,     5,     3,    24,    47,    83,  1197,\n",
       "           55,   116,   671,     8,    62, 15435,    11,  5817,     4,\n",
       "            6,  3348, 17801,    23,  5421,     7, 32085,     4,    68,\n",
       "           70,   799,   150,     8,  1407,    58,    52,  5530, 15671,\n",
       "           11,   285,     9,     6,  5005,     5,     3,     6, 17061,\n",
       "           47,   103,     6,  2093,    47,    83,    10,    12,   462,\n",
       "          219,    12,    15,   150,     5,     3,    62,   803,  2930,\n",
       "          980,     8,    41,   250,    10,   245,    19,   143,  1997,\n",
       "           20, 74705,     5,     3,    11,    10,  2042,    27, 74705,\n",
       "          249,    11, 13420,     4,     6, 17061,  9568,     6,  2093,\n",
       "           23,    95,    15,   166,    61,  7180,     8,   150,     7,\n",
       "           62,   877,     5,     3,    12,   104,   108,    18,   104,\n",
       "         1296,    15, 31723,    81,   104,   197,    18,   104,    33,\n",
       "          217,    11,    46,   186,   481,  2156,    30,   518,     7,\n",
       "           30,  7180,    30, 74705,     4,    12,   103,     6, 17061,\n",
       "            5,     3,    12,   104,   108,    18,   204,     9,   133,\n",
       "           40,  1382,    24,  8129,    81,    46,   385,     7,  4075,\n",
       "         1300,     9,  2576,   440,     8,    41,   250,   233,   503,\n",
       "            7,    50,    33,     8,   587,  3650,     5,     3,    12,\n",
       "           31,     6,  1157,     9,    46,   574,     4,  1149,    21,\n",
       "           15,   314,    46,   877,     7,    74,    61,  1409,    81,\n",
       "           50,  2080,   125,     4,    30,    21,    49,  1481,     9,\n",
       "         1722,   195,   143,     5,    12,     3,    55,   103,     6,\n",
       "         2093,    80,    83,    60,   102,   175,   387,     7,    70,\n",
       "          230,    28,    41,    12, 11576,   462,   219,    15,   133,\n",
       "          204,    12,     5,     3,    12, 31723,     7,   104,   196,\n",
       "          757,     8,  3491,   242,     7,   242,   105,     6,   540,\n",
       "          134,    22,    71,   251,     4,    12,    55,   469,     5,\n",
       "            3,     6, 17061,  4921,    30,    10,   385,     7,  4075,\n",
       "         3855,    11, 13450,   170,     4,   300,   430,    62,   589,\n",
       "           27,  4703,     6,  1033, 16186,     5,     3,    55,  2238,\n",
       "           58,    62,   102,  4075,    11, 15693,   170,   248,    62,\n",
       "          102,   276,  4925]), array([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_train[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[    6,  2177,   104,  1164,     3,  3935,    32,   678,    26,    10,\n",
      "           517,  2814,    18, 12677,    97,     6,  1385,    27,    54, 14420,\n",
      "         48253,     5,     3,    10, 71421,    19,   262,   273,     4,    31,\n",
      "          2051, 11426,     4,    47,    83,  3040,    20,  9395,     9,  5082,\n",
      "             8,   578,     4,  2613,  1326,     7,  1295,    22,  3912,  2257,\n",
      "             4,  1005,   103,     5,     3, 53184,  1005,   103,     6,   273,\n",
      "          1786,    11,  1005,  6191,     7,    51,    33,   591,    15,  6782,\n",
      "             8,   470,    90,     5,     3,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    6,  6056, 16364,    15, 78566,     7, 56039,    11,  2051, 27265,\n",
      "            32,  1516,    11,     3,     4,    20,   335, 13154,     5,     3,\n",
      "         20911, 27672,    80,   767,     6,  8206,    27,    10,  3568,    27,\n",
      "             6,   307,     9,  1215,    20,     6,     3,  1152,  1088,    27,\n",
      "         77066,  1841,  1735,     4,    11, 13567,  8210,     5,     3,    89,\n",
      "           863,     7,  1848,   422,   203,    11,    10,   531,  3568,     7,\n",
      "          8024,    10, 10310,  1750,     8,    89,     5,     3, 15404,     3,\n",
      "             4, 20911, 27672,    23,   943,     4,   103,    13,    12, 20737,\n",
      "            32,    41, 16364,     4,    10,  9172,     4,    10,   877,     4,\n",
      "            10,  1848,     7,    10,  1222,    44,   117,     9,    45,   115,\n",
      "            32,    10,  6828,     7,   115,    70,    33,    83,  5565,    27,\n",
      "             6,  1850,  1088,     5,     3,    12,   115,   913,   100,   342,\n",
      "            24,    32,     8,    33,  1687,    65,    29,    11,    71,   323,\n",
      "           460,     5,    12,     3,   115,    88,  4779,   730,    27,     6,\n",
      "          1735,    18,   204, 20911, 27672,     7,    89,  1848,     4, 14902,\n",
      "             4,    80,    83,   930,    82,     5,     3,    12,    29,    14,\n",
      "            46,   734,   529,     7,    20,     6,   178,   128,  4565,    50,\n",
      "           293,    22,   128,   792,  3457,     4,    12, 20911,     3,   103,\n",
      "             5,     3,    12,  3008,    82,   668,    42,    28,    10,   734,\n",
      "             4,    12,   115,   469,     4,     8,   131,  6964,     5,     3,\n",
      "         42682, 36869,     4,  4714,     9,     6,  4068, 27265,  4477,  1584,\n",
      "            68,  1379,     6,  1735,     4,   103,    13,    12,    81, 20737,\n",
      "           388,     7,   767,     6,   102,  8206,    24,    32,   230, 25215,\n",
      "           115,    70,   221,   136,     7,   374,     6,   219,     5,     3,\n",
      "            12,     8,    33,    18,  2701,  2293,   147,    89,   863,     7,\n",
      "          1848,    14,    76,    61, 19553,     5,    12,     3,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "1 tensor([[    6,   303,   601,   326, 45415,     3,     7,   326, 41676,     3,\n",
      "             4,   204,     9,    68,    25,   718,     8,    28,  1775,  2296,\n",
      "             4,     7,     6,   159,     8,   362,    10,   634,  2059,  6557,\n",
      "             5,     3,     6,  4458,   103,     6,   622,    32,     6,  1282,\n",
      "            11, 11712,  7880,   521,     5,     3,  1298,     9,     6,  6557,\n",
      "            14,   572,     8,  1117,    11, 10725,     5,     3,    12,   258,\n",
      "            14,    10,  3028,   154,    15, 31629,     4,    12,   103,     6,\n",
      "          4458,    23,  2327,  2262, 52382, 92947,     5,     3,    12,    50,\n",
      "            33,  4603,    71,  5732,  7218,    15,   134,     8,   221,     7,\n",
      "            25,   109,  2474,    22,     6,  5259,    22,   204, 45415,     7,\n",
      "         41676,     5,    12,     3, 31629,  4968, 11409,  5395,     8,   326,\n",
      "          5627,     7,    47,  1655,  9313,  1333,     5,     3,     6,   268,\n",
      "           290,    24,    14,     6,   301,  1282,  4458,    11, 51567,     7,\n",
      "             6,   734,  1282,     9,   101,   448,    11,  9868,     5,     3,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    6, 42689,    19,   262,  1713,    10,   719,  1406,   178,   143,\n",
      "             7,    47,  2040,   404,    15,     6, 11910,   102,   294,     4,\n",
      "            11,     6, 59802,  8187,   280, 49524,    11, 14110,     5,     3,\n",
      "         47324,    47,  2007,    83,   780,    11,    10, 10502,     9, 14482,\n",
      "         60330,  5207,     5,     3,     6, 11910,  9508, 13842,   805,    15,\n",
      "            10, 71701, 56176,   350,    18,   178,   454,  1826,     6,  5001,\n",
      "           719,   499,  2366,   292,    19, 20614,     5,     3,   164,    45,\n",
      "             6,   763,  1410,  6449,    20,    71,  1750,   277,    34,   529,\n",
      "            71,  5039,  2366, 13067,   125,     5,     3,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "2 tensor([[   10,   222,    27,     6, 93935,  1691,    32,  2070,    30,     6,\n",
      "          2913,   814,    26, 63416,     7, 52462,  3397,   178,   143,     5,\n",
      "             3,    24,   487,  2271,     8,   165,   757,   955,    15,     6,\n",
      "           117,  1593,     9,   245,   936,   851,    15,     6,   492,    19,\n",
      "           246,    10, 19450,     9,     3,  1082,     5,     3,   653,     4,\n",
      "            10,  3509,    15,  7351,  2118,     8,   285,     6,   492,    47,\n",
      "            37,  3775,  1131,     5,     3,     6,  3026,  4766,     8,   507,\n",
      "            10,    91,   175,    15,    10,  3061,   865,     9,  3028,  1762,\n",
      "           712,   706,    26,     6,  3397,    11,   656,  2033,     5,     3,\n",
      "             6,  3397,   103,     6,   499,     9,     6,  1965,   116,    88,\n",
      "          3180,     8,   323, 12964,     7,  2256,  4820,     5,     3,     6,\n",
      "           492,    32,   718,     8, 12212,     6,  3028,  1762,    19,    84,\n",
      "             9,    68,   600,   136,    30,   369,    30, 36301,    19,   194,\n",
      "            88,  3350,   356,   467,     8,     6,  5538,     5,     3,    10,\n",
      "           423, 11190,  1152,   366,  2178,    32,  2354,     8,     6,  1017,\n",
      "            15,     6,   499,    68,    32,   918,  1486,    26,     6,  3397,\n",
      "             5,     3,  1370,   621,   962,    86,  2963,    27,   423,  9563,\n",
      "             5,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [49139,    86,  6822,  5733,   376,   207,   154,   564,    96,    30,\n",
      "            41, 18858,     4,   103,     6, 21968,  2515,     7,  2562,   488,\n",
      "          4713,    17, 85656,    16,   562,     5,     3,     6,   602,     9,\n",
      "          8749, 49139,   227,     4,    20,   900,     4,    95,  7059,     6,\n",
      "          3167,     9,  1323,  5298,     9,  2136,    10,   143,     5,     3,\n",
      "          7617,  2712,  3483,    58,     9,   326,    11,     6, 85656,    23,\n",
      "          1257,  1653,  1514,  2311,     5,     3,    12,     6,  3446,    86,\n",
      "          2367, 30341,   524,    45,  8201,     4,    12,   103, 85656,   488,\n",
      "          1355, 26193, 33453,     3,     5,     3,    12,    69,    50,    86,\n",
      "         10690,    58,   562,  1057,    15,  1653,   345,  7617,    70,    98,\n",
      "            85,    10,  1360,     4,    12,   103, 26193,     3,     5,     3,\n",
      "         26193,     3,  2144,  5733,   376,    30,   168,  2307,    18,   132,\n",
      "            28,    41,  5371,  1890,     4,   156,    30,  2136,     4,  2344,\n",
      "             7,   806,   376,     5,     3,  6697,   992,    25,  3109,     8,\n",
      "          1891,    11,  7617,     5,     3,   498,   128,    11,   245, 16161,\n",
      "          2179,     7,    53,    11,   459,   304,    25,  8595,    34, 10236,\n",
      "             4,   794,     8,     6,   437,    23,  8061,     9,   375,     7,\n",
      "          4906,     5,     3, 49139,   613,     8,  1045,   344,  5733,   376,\n",
      "             7,  1045,  1679,  5350,     9,   376,     4,   103, 26193,     3,\n",
      "             5,     3,    12,    72,  5733,    73,   376,    14,    92,   679,\n",
      "            76,    41, 18858,    19,    24,    23,   407,  4643,     7, 49139,\n",
      "            25,  1580,    24,   195,     7,   207,   154,     4,    12,   115,\n",
      "           103,     5,     3,    12,    51,    88,   142,     8,    28,    52,\n",
      "         14694,     9,   207,  4648,    51,   165,    26,  1580,    52,  2486,\n",
      "             7, 13790,     5,    12,     3,     6,  2311,  6855,    10,   328,\n",
      "            23,  1653,   378,    20,   948,     4,  3171,     7,  4402,     9,\n",
      "             6,  1725,   376,   898,    30,   126,    30,   821,  5362,   156,\n",
      "            30,   597,     7,  4112,     5,     3,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "3 tensor([[63522,  1355,  8624,     3,  1984,    62,  1700,    40,  2654,     6,\n",
      "           985,     8,     6,   994,     9, 10559,    15,  2404,    17, 30621,\n",
      "            16,     5,     3, 74005,  2327,  9349,   193,   103, 63522,    23,\n",
      "          1862,   388,    30,    12,    10,  2210,    12,     5,     3,    41,\n",
      "         16161,  1410,  2366, 15757,    80,  5147,    24,    32,    37,    12,\n",
      "          7653,  3527,    12,    93,   788,    32,  6399,    10,  5335, 15243,\n",
      "          4009,     5,     3,    11,   253,     4,   193,    23, 16161,  1410,\n",
      "          2366,    17, 46012,    16,   294,    86,  7549,    26,     6, 16161,\n",
      "          1218,  6655,    19, 19879,  2236,    17, 47948,    16,    97,  5030,\n",
      "          6399,     8,   766,     6,  1033,   454,     5,     3, 74005,    86,\n",
      "            53,     9,   128, 16161,  1160,  1267,   410,    22,   872,   270,\n",
      "            19,  1727,  9074,  2366,  3079,    11,     6,   402,   220,  9692,\n",
      "           358,    10,   562,  5605, 19879,    11,  2404,    11,     6,   402,\n",
      "             5,     3,     6,  1190,    86,  3925, 12016, 16885,    59,    79,\n",
      "          1152,    17,   423,    67,  1152,    16,    26,     6, 46012,     7,\n",
      "          5335,  1629,  9112, 73620,    15,   223,   308,   358,    41,  8387,\n",
      "           562,    26, 47948,    18,  6478,   586, 23029,  1315,     6,  2724,\n",
      "            18,   766,   116,    33, 25027,     5,     3,    11,  4976,     4,\n",
      "         47948,   118,  6519,    20,  1858, 74005,   766,    19,   429,     7,\n",
      "           609,    19,    24,  2695,    86,  6179,  6399,    10,  5335, 15243,\n",
      "         74479,  4569,   124,     4,    68,  6985,  2693,   976,     4,    26,\n",
      "         74005,  1218,  6303, 22629, 31094,     5,     6,   766,    86,  4581,\n",
      "            10, 15438,  4544,  5967,    41, 46012, 15757,     5,     3,    18,\n",
      "         15757,    11,  5869,  8210,  5798,     6,   766,     4,   194, 47948,\n",
      "          9574,    37,     8,  2571,   280,     6,  8986,     5,     3, 31094,\n",
      "            32,    88,  5798,     9,   427, 17449,     9,     6,  2366,    23,\n",
      "          1169,   645,     4,    44,    32,   227,  3377,     9,   157,  1985,\n",
      "             9,    12,  9872,     4,  4838,     8,  1288,     7, 26310,    11,\n",
      "          2380,   654,     8,    10,   561,     9,  6414,  6598,    12,     5,\n",
      "         31094,    40,  2571,   280,   168,  1985,     5,     3,     0,     0],\n",
      "        [  517,     4,  5855,   872,   288,    11,     6,   190,   803,     4,\n",
      "           688,    11,    29,   143,    23,   190,  4323,  7441,   506,     8,\n",
      "         24298,    62,   560,   141,  2131,   134,     5,     3,    44, 70724,\n",
      "           103,     6,   759,   293,    19, 41833,     7, 55203,    70,    28,\n",
      "            12,  6193,   136,    15,     6,   233,   128,   134,    30,    10,\n",
      "          1881,    12,     5,     3, 67679, 55203,     4,  2665,     4,  1030,\n",
      "             6,   190,  4323,    11,  1746,     5,     3, 70724,    32,  2137,\n",
      "            30,   203,     9,     6, 21181,    23,   710,   134,     9,     6,\n",
      "         33532,  3339,     7,   103,    13,    12,   104,   116,    42,   130,\n",
      "             9,    10,  1140,    18,    32,    52,  1344,     8,  3879,     6,\n",
      "           985,     5,     3,    12,    51,    25,  1115,  4900,  3829,    11,\n",
      "            60,   192,   159,    44,   149, 19832,    15,     6,  2404,     9,\n",
      "         26531,     5,    12,     3,   517,     4,    10,  6612,     7,  3206,\n",
      "          4323,  2199,     4,  2489,    58,     9,     6,   270,  2152,   141,\n",
      "            62,   157,    19,   200,  4469,    26,  2465, 61036,  5414,     3,\n",
      "            11,     6,   102,  1009,     9,  7441,    27, 13074, 13932,    11,\n",
      "         33335,   178,   260,     5,     3,    44,   190, 26531,    23,   985,\n",
      "           400,    55,    40,    37,    33,     8,  1666,     8, 10112,    10,\n",
      "           276,  1300,   560,    27,   114,    23,  6952,   291,     5,     3,\n",
      "            12,  4187,   104,   181,    85,    24,   387,     7,   104,    43,\n",
      "          1386,  3191,     4,    61,   194,   104,   167,   181,   198,    18,\n",
      "           214,   104,   278,   461,     4,    12,     6, 69981,    19,   262,\n",
      "           103,     5,     3,    12,   104,   174, 19855,     7,   637,  9130,\n",
      "             7,     6,   214,   181, 25379,    74,     5,    12,     3,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "4 tensor([[ 2148,   309,    86,   361,     8,     6,  3324,   483,     3,    27,\n",
      "         28105, 48253,    20,  9957,     5,     3,    10, 30982, 11031,   244,\n",
      "         11271,   103,     6,   328,    32,   550,     8, 49035,    23,     3,\n",
      "          1735,    22,  2442,  3231,     5,     3,  4068,     7,  2051,   932,\n",
      "           244,   911,   128,  9012,    31, 77233,     7,    53,    31,     3,\n",
      "             8,     6,  1351,     5,     3,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [33698,  1583,     6, 24765,    11,    10,  3958,  1219,    30, 22726,\n",
      "          7612,    60, 17816,   391,     8,   261,   428,     5,     3, 22726,\n",
      "          1219,    60,   346,  1207,   280, 55585,    20, 17956,     4,   267,\n",
      "         13420,    17, 16471, 48253,    16,     7,     6,  3829,    40,   270,\n",
      "          1290,  2635,     3,     6,  8822,    19,  7118,    25,    20,  9075,\n",
      "             4,   266, 13420,    22,     6,   803,     6,   358,   154,     5,\n",
      "             3,     6,  4239,  1601,    60,  1279,    22,    10,   185,    19,\n",
      "            67,   655,   105, 32374,     5,     3,   617,  7986,    14,    37,\n",
      "          2139,    20,    29,   937,     3, 44307, 82006, 33698,   103,    13,\n",
      "            12,    24,    32,    10,  8115,   214,    15,    71,   203,     5,\n",
      "             3,    12,    50,   122,    42,   146,   292,    30,   126,    30,\n",
      "            50,   122,    11,     6,  1033,   214,     4,    44,    50,  1660,\n",
      "            10,   241,     9, 15334,   138,    50,  2637,     8,     6,   382,\n",
      "             5,     3,    12,    50,   913,  2652,  3607,     4,    69,    50,\n",
      "           198,     8,     6,  2176,  1799,   181,    27,    10,  1960,     4,\n",
      "           118,    50,    70,    28,   797,     7,    24,  1660,    11,     6,\n",
      "           250,     5,    12,     3,  5202, 38222,     3,     7, 39431, 46002,\n",
      "          1800, 22726,  4457,  5139, 22136, 35495,    20,    10,   739,  7858,\n",
      "          5888,     5,     3,     3,   469,    13,    12,    69,    50,    80,\n",
      "           744,    24, 11073,     4,    18,   257,    28,    78,    50,   122,\n",
      "            19,  4510,    15,     6,  8822,  7118,    22,    10,   214,     8,\n",
      "          4206,    19,    21,   286,    42,   146,   627,    15,   131,    52,\n",
      "            96,    18,     5,     3,    12,    44,    50,   108,    50,   167,\n",
      "           198,    41,  4528,   241,     8,   127,    20,    69,    50,   148,\n",
      "             8,    49,   126,    11,     6,  8822,  7118,     5,    12,     3,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "5 tensor([[   12,   370,   977,    18,    50,    33,    83,  2938,    26, 53031,\n",
      "            18, 34738,    40,    37,    28,  1179,    97,     6,   981,    20,\n",
      "             6,   154,     9,     6,  9789,     4,    12,   103,    10,  1294,\n",
      "            20, 64291,    23,   334,     5,     3,   128,   134,   209, 64291,\n",
      "          1058,  4278,  5262,     9,  1329,  3241,     7,  2871,    63, 35483,\n",
      "          1249,     6,  3558,    27, 53031,    81,   204,  1267,    86,    11,\n",
      "             6,  1380,     5,     3,    53,  1105,    32,   487,   227,  3377,\n",
      "             9, 22125,    10,  1410, 21445,    22,    41, 12895,  8686,     5,\n",
      "             3,     6,  1034,    32,   203,     9,    10,  4517,   141,    10,\n",
      "          1105,    32, 18740,    31,     6,   981,    27,  3251,   876, 36682,\n",
      "            15,  5082,     8,   444,   105,    41, 12895,  1410,     5,     3,\n",
      "         64291,     4,    75,    25,  3031,    11,  2366,   128,     4,    33,\n",
      "            83,  8159,  4269,  1435,    15, 53031,     4,    75,  1625,    53,\n",
      "           219,     7,  1465,   425,   662,     6, 34542,     5,    51,   292,\n",
      "            20, 10078,     4,   605,     3, 10887,    27, 16208, 48253,     5,\n",
      "             3, 64291,  1058,    33,    10,   205,   521,     9,   511, 34738,\n",
      "             8,   279,  4021,     5,     3,    11,  3039,     4,  1481,     9,\n",
      "            60,  4446, 12433,    20, 30764,  5628,    81, 64291,     4,   118,\n",
      "            11,     6,   262,  2176,  2854,     4,   805,   270,    19,  1727,\n",
      "         45736,    11,     6,  3699,  1009,     9,     6, 15673,  1675,     5,\n",
      "             3, 64291,   688,     6,   214,    99,    19,    67,    44,  5716,\n",
      "          6058,    15,    10,   830, 41947,     9, 12895,    57, 12765, 39476,\n",
      "            57,  1316,    68,  1728,     6, 17030,     5,     3, 64291,    23,\n",
      "           391,     8,     6, 64050,     9,     6,  1561,    80,   780,    10,\n",
      "           734,  1009,   655,   105,   270,    19,  1727, 60512,     5,     3,\n",
      "             6, 16033,     9,   511, 34738,     8,  1410,   981,   422,    10,\n",
      "           287,   835,   248,     6,  3252,    19,  3544,   454,     5,     3,\n",
      "         27921,   413,  1058,   422,  6488,  2871,    19,    63,  2322, 34684,\n",
      "             8, 18949,   413,    18,   454,     4,   194, 12895, 11034,    33,\n",
      "           407,  1255,    22,   413,     5,     3,   794,     8,   413,    23],\n",
      "        [16309,  9112,     4,  2384,     4,     9, 18077,   850,     4, 60516,\n",
      "             4,  2007,  4161, 21182,    27, 33740,  5418,   994,     5,     3,\n",
      "         14579,    12, 69466,    12, 81838,     4,    31, 60516,  1116,   141,\n",
      "           166,  3426,   404,    26,    62,   569,    30,    51,  2955,   175,\n",
      "           141,    10,   325,     9,  1483,  2604,     5,     3,    51,    80,\n",
      "           204,  2927,     6, 11669,  3451,  5078,     9,     6,     3, 11148,\n",
      "          1190,   483, 73007,    11, 45273,     5,     3,     6,  1650,     4,\n",
      "            75,    86,   204,   439,     9,     6, 76914, 25871, 11148,  1190,\n",
      "             4,    86,  3257,   136,     8,    60, 13198,    20,   139,  5869,\n",
      "             7,  1903,   173,     8,  7955,   823,     5,     3, 10655,  9112,\n",
      "           496,  1005,    62,   569,    80, 44822,    20,    62,  2273,     7,\n",
      "           103,    13,    12,   104, 12794,   150,     7,  6556,   150,    58,\n",
      "            12,     4,  5735,     6,  2871,    30,    10,    12,   193, 17951,\n",
      "            12,     5,     3,    55,   122,    37,   800,     8,   113,    78,\n",
      "           318, 10655, 81838,     4,    75,   217, 17617,  5375,     4,    32,\n",
      "            11,     4,     6,   994,   720,     5,     3,  2317, 24500, 46958,\n",
      "           103,    13,    12,    78,  1526,   132,    33,    83,    10,   973,\n",
      "             9,   414,     7,  6719,   954,    97,    29,  6286,     5,    12,\n",
      "             3, 10655, 81838,    23,  1552, 28394,   103,   141,     6,   313,\n",
      "            13,    12,    50,    43,   113,   109,   208,  2636,   151,    82,\n",
      "            70,    28,  3221, 24100,     8,  5995,    29,   438,     9, 20998,\n",
      "          1373,     5,    12,     3,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc1494496a8>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/c4lab/miniconda3/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/c4lab/miniconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/home/c4lab/miniconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 79, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 79, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 64, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 1 and 2 in dimension 2 at /pytorch/aten/src/TH/generic/THTensor.cpp:689\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-24a641b8ecbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 79, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 79, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 64, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 1 and 2 in dimension 2 at /pytorch/aten/src/TH/generic/THTensor.cpp:689\n"
     ]
    }
   ],
   "source": [
    "for idx,i in enumerate(trainLoader):\n",
    "    print(idx,i[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size=2\n",
    "extract_train=TrainData(trainData)\n",
    "extract_val=TrainData(valData)\n",
    "trainLoader=DataLoader(extract_train,batch_size=batch_size,num_workers=1)\n",
    "valLoader=DataLoader(extract_val,batch_size=batch_size,num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.633468724614135"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate positive negative ratio\n",
    "positive=0\n",
    "negative=0\n",
    "for i in extract_train.trainY:\n",
    "    \n",
    "    positive+=np.sum(i)\n",
    "    negative+=len(i)\n",
    "\n",
    "negative/positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 79, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 79, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 64, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 2 and 1 in dimension 2 at /pytorch/aten/src/TH/generic/THTensor.cpp:689\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-f17a1b747564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 79, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 79, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 64, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/c4lab/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 2 and 1 in dimension 2 at /pytorch/aten/src/TH/generic/THTensor.cpp:689\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "data_dim=embedding.shape[1]\n",
    "hidden_size=data_dim\n",
    "out_feat=2\n",
    "\n",
    "num_epoch=600\n",
    "lr=0.001\n",
    "pretrain=torch.Tensor(embedding)\n",
    "\n",
    "\n",
    "model=ExtractiveSeqTagging(hidden_size,pretrain).cuda()\n",
    "criterion=nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([11.633]).cuda())\n",
    "optimizer=optim.Adam(params=model.parameters(),lr=lr)\n",
    "# model=ExtractiveSeqTagging(30rameters(),lr=lr)\n",
    "loss=0\n",
    "best_acc=0\n",
    "\n",
    "\n",
    "fo = open(\"loss_train_adl.txt\", \"w\")\n",
    "\n",
    "for epoch in range(1,num_epoch+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss=0\n",
    "    val_loss=0\n",
    "    pred_arr=[]\n",
    "    cnt_train=0\n",
    "    cnt_val=0\n",
    "    model.train()\n",
    "    \n",
    "    for i,e in enumerate(trainLoader):\n",
    "        hidden=model.initHidden(e[0].shape[0])\n",
    "        optimizer.zero_grad()\n",
    "        out=model(e[0].cuda(),hidden)\n",
    "        loss=criterion(out.transpose(0,1),e[1].cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+=loss.item()\n",
    "        \n",
    "        result=np.argmax(out.detach().cpu(),2).T.numpy()\n",
    "        \n",
    "        batch_bounds,_=extract_train.get_bound(i,batch_size)\n",
    "        for num,bounds in enumerate(batch_bounds):            \n",
    "            prediction={}\n",
    "            candidate=[]\n",
    "            values=[]\n",
    "            for num2,j in enumerate(bounds):\n",
    "                sentence_cnt=np.sum(result[num,:][j[0]:j[1]])\n",
    "                if(sentence_cnt>0):\n",
    "                    candidate.append(num2)\n",
    "                    values.append(sentence_cnt)\n",
    "            prediction['id']=trainData[cnt_train]['id']\n",
    "            \n",
    "            prediction['predict_sentence_index']=[candidate[i] for i in np.argsort(values)]\n",
    "            cnt_train+=1\n",
    "            pred_arr.append(prediction)\n",
    "        progress = ('#' * int(float(i)/len(trainLoader)*40)).ljust(40)\n",
    "        print('[%03d|%03d] %2.2f sec(s) | %s |' %(epoch,num_epoch,time.time()-epoch_start_time,progress),end='\\r',flush=True)\n",
    "    target_word,prediction_word=predict2sentence(trainData,pred_arr)\n",
    "    scores_train=calculate_rouge_score(prediction_word,target_word)\n",
    "    \n",
    "    pred_arr=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,e in enumerate(valLoader):\n",
    "\n",
    "            hidden=model.initHidden(e[0].shape[0])\n",
    "            out=model(e[0].cuda(),hidden)\n",
    "            loss=criterion(out.transpose(0,1),e[1].cuda())\n",
    "            val_loss+=loss.item()\n",
    "            \n",
    "            result=np.argmax(out.detach().cpu(),2).T.numpy()\n",
    "        \n",
    "            batch_bounds,_=extract_val.get_bound(i,batch_size)\n",
    "            \n",
    "            for num,bounds in enumerate(batch_bounds):            \n",
    "                prediction={}\n",
    "                candidate=[]\n",
    "                values=[]\n",
    "                for num2,j in enumerate(bounds):\n",
    "                    sentence_cnt=np.sum(result[num,:][j[0]:j[1]])\n",
    "                    if(sentence_cnt>0):\n",
    "                        candidate.append(num2)\n",
    "                        values.append(sentence_cnt)\n",
    "                prediction['id']=valData[cnt_val]['id']\n",
    "\n",
    "                prediction['predict_sentence_index']=[candidate[i] for i in np.argsort(values)]\n",
    "                cnt_val+=1\n",
    "                pred_arr.append(prediction)\n",
    "                \n",
    "            progress = ('#' * int(float(i)/len(trainLoader)*40)).ljust(40)\n",
    "            print('[%03d|%03d] %2.2f sec(s) | %s |' %(epoch,num_epoch,time.time()-epoch_start_time,progress),end='\\r',flush=True)\n",
    "    \n",
    "    target_word,prediction_word=predict2sentence(valData,pred_arr)\n",
    "    scores_val=calculate_rouge_score(prediction_word,target_word)\n",
    "    \n",
    "    fo.write('[%03d|%03d] %2.2f sec(s) | train loss: %2.5f | rouge-1: %2.4f | rouge-2: %2.4f |rouge-l: %2.4f |val loss: %2.5f | rouge-1: %2.4f | rouge-2: %2.4f |rouge-l: %2.4f \\n' \\\n",
    "          %(epoch,num_epoch,time.time()-epoch_start_time,train_loss/len(trainLoader.sampler),scores_train['mean']['rouge-1']\\\n",
    "            ,scores_train['mean']['rouge-2'],scores_train['mean']['rouge-l'],val_loss/len(valLoader.sampler),\\\n",
    "            scores_val['mean']['rouge-1'],scores_val['mean']['rouge-2'],scores_val['mean']['rouge-l'] \\\n",
    "            ))\n",
    "    \n",
    "    print('[%03d|%03d] %2.2f sec(s) | train loss: %2.5f | rouge-1: %2.4f | rouge-2: %2.4f |rouge-l: %2.4f |val loss: %2.5f | rouge-1: %2.4f | rouge-2: %2.4f |rouge-l: %2.4f' \\\n",
    "          %(epoch,num_epoch,time.time()-epoch_start_time,train_loss/len(trainLoader.sampler),scores_train['mean']['rouge-1']\\\n",
    "            ,scores_train['mean']['rouge-2'],scores_train['mean']['rouge-l'],val_loss/len(valLoader.sampler),\\\n",
    "            scores_val['mean']['rouge-1'],scores_val['mean']['rouge-2'],scores_val['mean']['rouge-l'] \\\n",
    "            ))\n",
    "    \n",
    "fo.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5494, -0.9779, -0.4602,  ...,  0.0237, -0.2096,  0.3666],\n",
       "        [ 0.6949,  0.9781,  0.8065,  ..., -0.1200, -0.3407, -0.6864],\n",
       "        [ 0.0671, -0.2315, -0.0094,  ..., -0.3814, -0.0147, -0.0139],\n",
       "        ...,\n",
       "        [-0.5142, -0.2361, -0.0353,  ..., -0.2072, -0.6076,  1.0162],\n",
       "        [ 0.2020, -0.5053,  0.1782,  ..., -0.0385,  0.1093, -0.1146],\n",
       "        [ 0.2228, -0.2964,  0.6941,  ...,  0.0939, -0.1655, -0.3459]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
